{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Preprocessing Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, FunctionTransformer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| variable     | type       | description                   |\n",
    "|--------------|------------|-------------------------------|\n",
    "| mpg          | continuous | miles per gallon              |\n",
    "| cylinders    | discrete   | number of cylinders of engine |\n",
    "| displacement | continuous | engine volume                 |\n",
    "| horsepower   | continuous | obvious                       |\n",
    "| weight       | continuous | weight in kilos               |\n",
    "| acceleration | continuous | obvious                       |\n",
    "| model_year   | discrete   | obvious                       |\n",
    "| origin       | discrete   | 1. US, 2. EU, 3. ASIA         |\n",
    "| name         | string     | unique name of the automobile |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "COL_NAMES = ['mpg', \n",
    "             'cylinders',\n",
    "             'displacement',\n",
    "             'horsepower',\n",
    "             'weight',\n",
    "             'acceleration',\n",
    "             'model_year',\n",
    "             'origin',\n",
    "             'name']\n",
    "\n",
    "auto_data = pd.read_csv(DATA_PATH, delim_whitespace=True, names=COL_NAMES, na_values='?')\n",
    "train_auto_data, test_auto_data = train_test_split(auto_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Preparation / Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to transform our data for some algorithms since some algorithms work a lot better with standardized data, some don't work at all with missing data.\n",
    "\n",
    "Some algorithms may give a lot more weight to features which are not standardized or scaled.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple example of standardization with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.01577506,  0.92460222],\n",
       "       [ 0.71805744, -1.40551842],\n",
       "       [-0.7302584 , -0.38024542],\n",
       "       ...,\n",
       "       [-1.98271816,  1.45922598],\n",
       "       [ 0.02933269,  1.61895566],\n",
       "       [ 0.25205936, -1.18726757]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic feature engineering with sklearn example\n",
    "skscaler = StandardScaler()\n",
    "x1 = (np.random.randn(1000)) * np.sqrt(10) + 43\n",
    "x2 = (np.random.randn(1000)) * np.sqrt(13) - 10\n",
    "d = pd.DataFrame({'x1': x1, 'x2': x2}) # create an example dataframe\n",
    "\n",
    "skscaler.fit(d)\n",
    "d_standardized = skscaler.transform(d)\n",
    "# d_standardized.mean() # or std\n",
    "\n",
    "# another (shorter) way is with fit_transform\n",
    "skscaler.fit_transform(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another example with polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another example with poly transform\n",
    "poly = PolynomialFeatures(include_bias=False)\n",
    "\n",
    "poly.fit_transform(d);\n",
    "# result feature matrix would be [1, a, b, a^2, ab, b^2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples of a custom transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use custom function\n",
    "function_transformer = FunctionTransformer(func=np.exp, inverse_func=np.log)\n",
    "transformed = function_transformer.fit_transform(d)\n",
    "function_transformer.inverse_transform(transformed); # this will get it back to the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkNA(method):\n",
    "    def wrapper(self, X):\n",
    "        if np.isnan(X).any():\n",
    "            raise Exception('There are missing values in the data')\n",
    "        else:\n",
    "            method(self, X)\n",
    "    return wrapper\n",
    "\n",
    "class CustomStandardScaler(TransformerMixin, BaseEstimator):\n",
    "\n",
    "    @checkNA\n",
    "    def fit(self, X, y=None):\n",
    "        X = self.validate(X, estimator=self)\n",
    "        self.means = np.mean(X, axis=0)\n",
    "        self.vars = np.var(X, axis=0)\n",
    "        self.scale = np.sqrt(self.vars)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = self.validate_data(X, estimator=self)\n",
    "        shifted_X = X - self.means\n",
    "        scaled_and_shifted_X = X / self.scale\n",
    "        return scaled_and_shifted_X\n",
    "\n",
    "scaler = CustomStandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransformer():\n",
    "    def fit(X, y=None):\n",
    "        pass\n",
    "\n",
    "    def transform(X):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Pipeline example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('polynomization', PolynomialFeatures(include_bias=False)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "pipe.fit(auto_data[['displacement', 'weight']], auto_data['mpg'])\n",
    "\n",
    "# you can pass different X data here for the test data\n",
    "predicted = pipe.predict(auto_data[['displacement', 'weight']]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoder Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "one_hot_encoder.fit_transform(auto_data[['origin']]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature union use case example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./feature-union.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "baby_pipe = FeatureUnion([\n",
    "    ('numerical',\n",
    "    Pipeline([\n",
    "        ('select_numerical', FunctionTransformer(func = lambda X: X.loc[:, ['displacement', 'weight']])),\n",
    "       ('poly', PolynomialFeatures(include_bias=False)),\n",
    "       ('scaler', StandardScaler())\n",
    "    ])),\n",
    "    ('categorical', \n",
    "    Pipeline([\n",
    "        ('pass_categorical', FunctionTransformer(func = lambda X: X.loc[:, ['cylinders', 'origin', 'model_year']])),\n",
    "        ('onehot', OneHotEncoder(sparse=False, handle_unknown='ignore'))\n",
    "    ]))\n",
    "])\n",
    "\n",
    "# baby_pipe.fit_transform(auto_data);\n",
    "\n",
    "super_pipe = Pipeline([\n",
    "    ('baby_pipe', baby_pipe),\n",
    "    ('model', LinearRegression()) # here we can just change the model we are using with another one\n",
    "])\n",
    "super_pipe.fit(auto_data, auto_data['mpg'])\n",
    "super_pipe.predict(auto_data); # or pass some test data and compare the differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
